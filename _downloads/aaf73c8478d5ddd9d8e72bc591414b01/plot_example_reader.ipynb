{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Creating a Reader\n\n**Suhas Somnath**\n\n8/28/2020\n\nThis document illustrates an example of extracting data out of proprietary raw\ndata files, thereby describing how one would write a ``sidpy.Reader`` class.\n\nThe captured information would be populated into a / set of ``sidpy.Dataset``\nobject(s) as appropriate.\n\n## Introduction\nIn most scientific disciplines, commercial instruments tend to write the data and metadata out into proprietary file\nformats that significantly impede access to the data and metadata, thwart sharing of data and correlation of data from\nmultiple instruments, and complicate long-term archival, among other things. One of the data wrangling steps in science\nis the extraction of the data and metadata out of the proprietary file formats and writing the information into files\nthat are easier to access, share, etc. The overwhelming part of this data wrangling effort is in investigating how to\nextract the data and metadata into memory. Often, the data and parameters in these files are **not** straightforward to\naccess. In certain cases, additional / dedicated software packages are necessary to access the data while in many other\ncases, it is possible to extract the necessary information from built-in **numpy** or similar python packages included\nwith **anaconda**. Once the information is accessible in the computer memory, such as in the\nform of numpy arrays, scientists have a wide variety of tools to write the data out into files.\n\nSimpler data such as images or single spectra can easily be written into plain text files. Simple or complex / large /\nmultidimensional data can certainly be stored as numpy data files. However, there are significant drawbacks to writing\ndata into non-standardized structures or file formats. First, while the structure of the data and metadata may be\nintuitive for the original author of the data, that may not be the case for another researcher. Furthermore, such\nformatting may change from a day-to-day basis. As a consequence, it becomes challenging to develop code that can accept\nsuch data whose format keeps changing.\n\nOne solution to these challenges is to write the data out into standardized files such as ``h5USID`` files.\nThe USID model aims to make data access, storage, curation, etc. simply by storing the data along with all\nrelevant parameters in a single file (HDF5 for now).\n\nThe process of copying data from the original format to **h5USID** files is called\n**Translation** and the classes available in pyUSID and children packages such as pycroscopy that perform these\noperation are called **Translators**.\n\nAs we alluded to earlier, the process of developing a ``sidpy.Reader`` can be\nbroken down into two basic components:\n\n1. Extracting data and metadata out of the proprietary file format\n2. Populating one or more ``sidpy.Dataset`` objects as necessary\n\nThis process is the same regardless of the origin, complexity, or size of the scientific data. It is not necessary that\nthe two components be disjoint - there are many situations where both components may need to happen simultaneously\nespecially when the data sizes are very large.\n\nThe goal of this document is to demonstrate how one would extract data and parameters from a Scanning Tunnelling\nSpectroscopy (STS) raw data file obtained from an Omicron Scanning Tunneling Microscope (STM).\nIn this dataset, a spectra was collected for each position in a two-dimensional grid of spatial locations, thereby\nresulting in a 3D dataset.\n\nThe code in this example is an abbreviation of the\n`AscTranslator <https://github.com/pycroscopy/pycroscopy/blob/master/pycroscopy/io/translators/omicron_asc.py>`_\navailable in our sister package - `pycroscopy`.\n\n### Recommended pre-requisite reading\n\nBefore proceeding with this example, we recommend learning about ``Sidpy.Reader``:\n\n.. tip::\n    You can download and run this document as a Jupyter notebook using the link at the bottom of this page.\n\n### Import all necessary packages\nThere are a few setup procedures that need to be followed before any code is written. In this step, we simply load a\nfew python packages that will be necessary in the later steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Ensure python 3 compatibility:\nfrom __future__ import division, print_function, absolute_import, unicode_literals\n\n# The package for accessing files in directories, etc.:\nimport os\nimport zipfile\n\n# Warning package in case something goes wrong\nfrom warnings import warn\nimport subprocess\nimport sys\n\n\ndef install(package):\n    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package])\n# Package for downloading online files:\ntry:\n    # This package is not part of anaconda and may need to be installed.\n    import wget\nexcept ImportError:\n    warn('wget not found.  Will install with pip.')\n    import pip\n    install(wget)\n    import wget\n\n# The mathematical computation package:\nimport numpy as np\n\n# The package used for creating and manipulating HDF5 files:\nimport h5py\n\n# Packages for plotting:\nimport matplotlib.pyplot as plt\n\n# import sidpy - supporting package for pyUSID:\ntry:\n    import sidpy\nexcept ImportError:\n    warn('sidpy not found.  Will install with pip.')\n    import pip\n    install('sidpy')\n    import sidpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Procure the Raw Data file\nHere we will download a compressed data file from Github and unpack it:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/pycroscopy/pyUSID/master/data/STS.zip'\nzip_path = 'STS.zip'\nif os.path.exists(zip_path):\n    os.remove(zip_path)\n_ = wget.download(url, zip_path, bar=None)\n\nzip_path = os.path.abspath(zip_path)\n# figure out the folder to unzip the zip file to\nfolder_path, _ = os.path.split(zip_path)\nzip_ref = zipfile.ZipFile(zip_path, 'r')\n# unzip the file\nzip_ref.extractall(folder_path)\nzip_ref.close()\n# delete the zip file\nos.remove(zip_path)\n\ndata_file_path = 'STS.asc'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Extracting data and metadata from proprietary files\n1.1 Explore the raw data file\n=============================\n\nInherently, one may not know how to read these ``.asc`` files. One option is to try and read the file as a text file\none line at a time.\n\nIf one is lucky, as in the case of these ``.asc`` files, the file can be read like conventional text files.\n\nHere is how we tested to see if the ``asc`` files could be interpreted as text files. Below, we read just the first 10\nlines in the file\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with open(data_file_path, 'r') as file_handle:\n    for lin_ind in range(10):\n        print(file_handle.readline().replace('\\n', ''))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Read the contents of the file\nNow that we know that these files are simple text files, we can manually go through the file to find out which lines\nare important, at what lines the data starts etc.\nManual investigation of such ``.asc`` files revealed that these files are always formatted in the same way. Also, they\ncontain instrument- and experiment-related parameters in the first ``403`` lines and then contain data which is\narranged as one pixel per row.\n\nSTS experiments result in 3 dimensional datasets ``(X, Y, current)``. In other words, a 1D array of current data (as a\nfunction of excitation bias) is sampled at every location on a two dimensional grid of points on the sample.\nBy knowing where the parameters are located and how the data is structured, it is possible to extract the necessary\ninformation from these files.\n\nSince we know that the data sizes (<200 MB) are much smaller than the physical memory of most computers, we can start\nby safely loading the contents of the entire file to memory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Reading the entire file into memory\nwith open(data_file_path, 'r') as file_handle:\n    string_lines = file_handle.readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Extract the metadata\nIn the case of these ``.asc`` files, the parameters are present in the first few lines of the file. Below we will\ndemonstrate how we parse the first 17 lines to extract some very important parameters. Note that there are several\nother important parameters in the next 350 or so lines. However, in the interest of brevity, we will focus only on the\nfirst few lines of the file. The interested reader is recommended to read the ``ASCTranslator`` available in\n``pycroscopy`` for more complete details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Preparing an empty dictionary to store the metadata / parameters as key-value pairs\nparm_dict = dict()\n\n# Reading parameters stored in the first few rows of the file\nfor line in string_lines[3:17]:\n    # Remove the hash / pound symbol, if any\n    line = line.replace('# ', '')\n    # Remove new-line escape-character, if any\n    line = line.replace('\\n', '')\n    # Break the line into two parts - the parameter name and the corresponding value\n    temp = line.split('=')\n    # Remove spaces in the value. Remember, the value is still a string and not a number\n    test = temp[1].strip()\n    # Now, attempt to convert the value to a number (floating point):\n    try:\n        test = float(test)\n        # In certain cases, the number is actually an integer, check and convert if it is:\n        if test % 1 == 0:\n            test = int(test)\n    except ValueError:\n        pass\n    parm_dict[temp[0].strip()] = test\n\n# Print out the parameters extracted\nfor key in parm_dict.keys():\n    print(key, ':\\t', parm_dict[key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point, we recommend reformatting the parameter names to standardized nomenclature.\nWe realize that the materials imaging community has not yet agreed upon standardized nomenclature for metadata.\nTherefore, we leave this as an optional, yet recommended step.\nFor example, in pycroscopy, we may categorize the number of rows and columns in an image under ``grid`` and\ndata sampling parameters under ``IO``.\nAs an example, we may rename ``x-pixels`` to ``positions_num_cols`` and ``y-pixels`` to ``positions_num_rows``.\n\n### 1.4 Extract parameters that define dimensions\nJust having the metadata above and the main measurement data is insufficient to fully describe experimental data.\nWe also need to know how the experimental parameters were varied to acquire the multidimensional dataset at hand.\nIn other words, we need to answer how the grid of locations was defined and how the bias was varied to acquire the\ncurrent information at each location. This is precisely what we will do below.\n\nSince, we did not parse the entire list of parameters present in the file above, we will need to make some up.\nPlease refer to the formal ``ASCTranslator`` to see how this step would have been different.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_rows = int(parm_dict['y-pixels'])\nnum_cols = int(parm_dict['x-pixels'])\nnum_pos = num_rows * num_cols\nspectra_length = int(parm_dict['z-points'])\n\n# We will assume that data was collected from -3 nm to +7 nm on the Y-axis or along the rows\ny_vec = np.linspace(-3, 7, num_rows, endpoint=True)\n\n# We will assume that data was collected from -5 nm to +5 nm on the X-axis or along the columns\nx_vec = np.linspace(-5, 5, num_cols, endpoint=True)\n\n# The bias was sampled from -1 to +1 V in the experiment. Here is how we generate the Bias axis:\nbias_vec = np.linspace(-1, 1, spectra_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Extract the data\nWe have observed that the data in these ``.asc`` files are consistently present after the first ``403`` lines of\nparameters. Using this knowledge, we need to populate a data array using data that is currently present as text lines\nin memory (from step 2).\n\nThese ``.asc`` file store the 3D data (X, Y, spectra) as a 2D matrix (positions, spectra). In other words, the spectra\nare arranged one below another. Thus, reading the 2D matrix from top to bottom, the data arranged column-by-column,\nand then row-by-row So, for simplicity, we will prepare an empty 2D numpy array to store the data as it exists in the\nraw data file.\n\nRecall that in step 2, we were lucky enough to read the entire data file into memory given its small size.\nThe data is already present in memory as a list of strings that need to be parsed as a matrix of numbers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_headers = 403\n\nraw_data_2d = np.zeros(shape=(num_pos, spectra_length), dtype=np.float32)\n\n# Iterate over ever measurement position:\nfor pos_index in range(num_pos):\n    # First, get the correct (string) line corresponding to the current measurement position.\n    # Recall that we would need to skip the many header lines to get to the data\n    this_line = string_lines[num_headers + pos_index]\n    # Each (string) line contains numbers separated by tabs (``\\t``). Let us break the line into several shorter strings\n    # each containing one number. We will ignore the last entry since it is empty.\n    string_spectrum = this_line.split('\\t')[:-1]  # omitting the new line\n    # Now that we have a list of numbers represented as strings, we need to convert this list to a 1D numpy array\n    # the converted array is set to the appropriate position in the main 2D array.\n    raw_data_2d[pos_index] = np.array(string_spectrum, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the data is so large that it cannot fit into memory, we would need to read data one (or a few) position(s) at a\ntime, process it (e.g. convert from string to numbers), and write it to the HDF5 file without keeping much or any data\nin memory.\n\nThe three-dimensional dataset (``Y``, ``X``, ``Bias``) is currently represented as a two-dimensional array:\n(``X`` * ``Y``, ``Bias``). To make it easier for us to understand and visualize, we can turn it into a 3D array:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw_data_3d = raw_data_2d.reshape(num_rows, num_cols, spectra_length)\nprint('Shape of 2D data: {}, Shape of 3D data: {}'.format(raw_data_2d.shape, raw_data_3d.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just as we did for the parameters (``X``, ``Y``, and ``Bias``) that were varied in the experiment,\nwe need to specify the quantity that is recorded from the sensors / detectors, units, and what the data\nrepresents:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_data_name = 'STS'\nmain_qty = 'Current'\nmain_units = 'nA'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize the extracted data\nHere is a visualization of the current-voltage spectra at a few locations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = sidpy.plot_utils.plot_curves(bias_vec, raw_data_2d, num_plots=9,\n                                        x_label='bias (V)',\n                                        y_label=main_qty + '(' + main_units + ')',\n                                        title='Current-Voltage Spectra at different locations',\n                                        fig_title_yoffset=1.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is a visualization of spatial maps at different bias values\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = sidpy.plot_utils.plot_map_stack(raw_data_3d, reverse_dims=True, pad_mult=(0.15, 0.15),\n                                           title='Spatial maps of current at different bias', stdevs=2,\n                                           color_bar_mode='single', num_ticks=3, x_vec=x_vec, y_vec=y_vec,\n                                           evenly_spaced=True, fig_mult=(3, 3), title_yoffset=0.95)\n\nfor axis, bias_ind in zip(axes, np.linspace(0, len(bias_vec), 9, endpoint=False, dtype=np.uint)):\n    axis.set_title('Bias = %3.2f V' % bias_vec[bias_ind])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Populating the ``Dataset`` object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_set = sidpy.Dataset.from_array(raw_data_3d, name='Raw_Data')\nprint(data_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dimensions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_set.set_dimension(0, sidpy.Dimension('y', y_vec,units='nm',\n                                          quantity='Length',\n                                          dimension_type='spatial'))\ndata_set.set_dimension(1, sidpy.Dimension('x', x_vec, units='nm',\n                                          quantity='Length',\n                                          dimension_type='spatial'))\ndata_set.set_dimension(2, sidpy.Dimension('bias', bias_vec,\n                                          quantity='Bias',\n                                          dimension_type='spectral'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generic top level metadata can be added as you go along\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_set.data_type = 'Current'\ndata_set.units = main_units\ndata_set.quantity = main_qty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instrument-specific metadata\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_set.metadata = parm_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the dataset object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(data_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the dataset object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_set.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How does one turn such ad-hoc code into a ``Reader`` class?\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### More information\nOur sister class - pycroscopy, has several\n`translators <https://github.com/pycroscopy/pycroscopy/tree/master/pycroscopy/io/translators>`_ that translate popular\nfile formats generated by nanoscale imaging instruments.\nThese will be moved to ScopeReaders soon\n\nWe have found python packages online to open a few proprietary file formats and have written translators using these\npackages. If you are having trouble reading the data in your files and cannot find any packages online, consider\ncontacting the manufacturer of the instrument which generated the data in the proprietary format for help.\n\n### Cleaning up\nRemove the original file:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "os.remove(data_file_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}